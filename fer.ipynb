{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOhpobYtFpZM",
        "colab_type": "text"
      },
      "source": [
        "# **Модель, распознающая эмоции**\n",
        "Здесь описано создание и обучение модели, распознающей эмоции по фотографии.\n",
        "\n",
        "## Эмоции\n",
        "Модель распознает 7 базовых эмоций по Экману:\n",
        "- злость (ANGER)\n",
        "- отвращение (DISGUST)\n",
        "- страх (FEAR)\n",
        "- счастье (HAPPINESS)\n",
        "- грусть (SAD)\n",
        "- удивление (SURPRISE)\n",
        "- нейтральное выражение (NEUTRAL)\n",
        "\n",
        "Инициализируем массив эмоций"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuDUTKcqtZdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emotions = [(0, \"ANGER\"), (1, \"DISGUST\"), (2, \"FEAR\"), (3, \"HAPPINESS\"), \n",
        "            (4, \"SAD\"), (5, \"SURPRISE\"), (6, \"NEUTRAL\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG_i79DtG_a5",
        "colab_type": "text"
      },
      "source": [
        "## Датасет\n",
        "Для обучения, будем использовать датасет fer2013. Найти его можно [здесь](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data). Загрузите его и поместите в папку с этим файлом.\n",
        "Или можно примонтировать Google Drive, если используется Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qZdJ46MOzUF",
        "colab_type": "code",
        "outputId": "55e8db0d-7802-45dd-a216-cf41504a1f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Монтирование Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1KRXfLTOCmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Путь до датасета, укажите свой\n",
        "fer_dataset_path = \"/content/drive/My Drive/university/reko/fer2013.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f5g4zsBxOAK",
        "colab_type": "text"
      },
      "source": [
        "## Подготовка данных\n",
        "Для обучения подготавливаем данные. Грузим датасет. Изображения в датасете имеют размер 48х48 пикселей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVDVWxJexf3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azWeWjyGxdcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "width, height = 48, 48\n",
        "data = pd.read_csv(fer_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKQiZ-2ux_qU",
        "colab_type": "text"
      },
      "source": [
        "Данные в датасете хранятся в виде векторов целых чисел. В столбце emotion указан идентификатор эмоции (ассоциативный массив определен выше), а pixels - это 2304 (48х48) чисел изображения, пиксели. Пример как это хранится внутри csv файла ниже:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McJYcDE_8EjQ",
        "colab_type": "code",
        "outputId": "3a4f889c-3a56-4572-d561-3cf71f8c3075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "data[:5]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGT6v7EYyfoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapoints = data['pixels'].tolist()\n",
        "\n",
        "all_images = []\n",
        "for image_xs in datapoints:\n",
        "  # преобразуем из строки в массив чисел\n",
        "  xx = [int(image_x) for image_x in image_xs.split(' ')]\n",
        "  # преобразуем одномерный массив в матрицу\n",
        "  xx = np.asarray(xx).reshape(width, height)\n",
        "  # добавляем изображение в список всех изображений\n",
        "  all_images.append(xx.astype('float32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YlXieUK018T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# преобразование к массиву numpy\n",
        "all_images = np.asarray(all_images)\n",
        "all_images = np.expand_dims(all_images, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lubnHi_VKyhz",
        "colab_type": "text"
      },
      "source": [
        "Считываем эмоции:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgaSHs90K1sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "29a1d8b5-cdea-41f7-bec6-f78f745a5b00"
      },
      "source": [
        "all_emotions = pd.get_dummies(data['emotion'])\n",
        "np.array(all_emotions)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjtFAINFwV6s",
        "colab_type": "text"
      },
      "source": [
        "## Модель\n",
        "Для распознавания эмоций на изображении лучше всего использовать сверточную нейронную сеть. Для построения модели будем использовать Keras и TensorFlow. Делаем нужные импорты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyA5M9ETwtI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KriClua_G-2o",
        "colab_type": "text"
      },
      "source": [
        "Так же задаем нужные константы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asnV8grGHCdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_features = 64 # количество признаков\n",
        "num_labels = len(emotions) # количество эмоций\n",
        "batch_size = 64 # каким размером партии данных обучать\n",
        "epochs = 100 # количество эпох, сколько раз обучать"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqcMyEYCH9kN",
        "colab_type": "text"
      },
      "source": [
        "Разделяем данные для теста и обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r1ba_gYIAce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "20a914a1-cf8a-4b2e-ece0-fa3b3f76acd1"
      },
      "source": [
        "separate_index = int(len(all_images) * 0.9)\n",
        "test_images, train_images = all_images[separate_index:], all_images[:separate_index]\n",
        "test_emotions, train_emotions = all_emotions[separate_index:], all_emotions[:separate_index]\n",
        "\n",
        "print(\"Размер тестовой подборки: {0}\".format(len(test_images)))\n",
        "print(\"Размер обучающей подборки: {0}\".format(len(train_images)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Размер тестовой подборки: 3589\n",
            "Размер обучающей подборки: 32298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw3VwQloJZy8",
        "colab_type": "text"
      },
      "source": [
        "Сама сверточная нейронная сеть построена следующим образом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhMZBluRJg_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e55d9c7-d8a4-4235-c898-e6c3cf288fef"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
        "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(2*2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*num_features, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_labels, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 46, 46, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 46, 46, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 46, 46, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 23, 23, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 23, 23, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 23, 23, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 23, 23, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 11, 11, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 5, 5, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 5, 5, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 5, 5, 512)         2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 5,905,863\n",
            "Trainable params: 5,902,151\n",
            "Non-trainable params: 3,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ9wWEYmJ8ui",
        "colab_type": "text"
      },
      "source": [
        "Модель создана, теперь следует её скопилировать и обучить."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdjI832J7tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtJMXYoeLqar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dfb19f7d-e0ea-48dd-8915-b9d1700c3fe2"
      },
      "source": [
        "model.fit(train_images, train_emotions,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(test_images, test_emotions),\n",
        "          shuffle=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32298 samples, validate on 3589 samples\n",
            "Epoch 1/100\n",
            "32298/32298 [==============================] - 80s 2ms/step - loss: 2.0393 - accuracy: 0.2144 - val_loss: 1.8380 - val_accuracy: 0.2452\n",
            "Epoch 2/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.8358 - accuracy: 0.2475 - val_loss: 1.8243 - val_accuracy: 0.2449\n",
            "Epoch 3/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.7979 - accuracy: 0.2679 - val_loss: 1.7554 - val_accuracy: 0.2917\n",
            "Epoch 4/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.7117 - accuracy: 0.3074 - val_loss: 2.0654 - val_accuracy: 0.1948\n",
            "Epoch 5/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.5838 - accuracy: 0.3707 - val_loss: 1.5552 - val_accuracy: 0.3918\n",
            "Epoch 6/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 1.4993 - accuracy: 0.4152 - val_loss: 1.4685 - val_accuracy: 0.4361\n",
            "Epoch 7/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.4410 - accuracy: 0.4453 - val_loss: 1.5077 - val_accuracy: 0.4207\n",
            "Epoch 8/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.3909 - accuracy: 0.4703 - val_loss: 1.2956 - val_accuracy: 0.4898\n",
            "Epoch 9/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.3465 - accuracy: 0.4906 - val_loss: 1.3024 - val_accuracy: 0.5007\n",
            "Epoch 10/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.3038 - accuracy: 0.5071 - val_loss: 1.2205 - val_accuracy: 0.5336\n",
            "Epoch 11/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.2789 - accuracy: 0.5220 - val_loss: 1.2439 - val_accuracy: 0.5266\n",
            "Epoch 12/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 1.2509 - accuracy: 0.5348 - val_loss: 1.1708 - val_accuracy: 0.5587\n",
            "Epoch 13/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.2269 - accuracy: 0.5449 - val_loss: 1.1994 - val_accuracy: 0.5589\n",
            "Epoch 14/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 1.1992 - accuracy: 0.5558 - val_loss: 1.1139 - val_accuracy: 0.5823\n",
            "Epoch 15/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.1834 - accuracy: 0.5654 - val_loss: 1.1037 - val_accuracy: 0.5754\n",
            "Epoch 16/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 1.1503 - accuracy: 0.5784 - val_loss: 1.0851 - val_accuracy: 0.6007\n",
            "Epoch 17/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 1.1310 - accuracy: 0.5896 - val_loss: 1.1538 - val_accuracy: 0.5651\n",
            "Epoch 18/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 1.1101 - accuracy: 0.5991 - val_loss: 1.0412 - val_accuracy: 0.6099\n",
            "Epoch 19/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 1.0822 - accuracy: 0.6105 - val_loss: 1.1067 - val_accuracy: 0.5913\n",
            "Epoch 20/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.0625 - accuracy: 0.6181 - val_loss: 1.0412 - val_accuracy: 0.6108\n",
            "Epoch 21/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.0435 - accuracy: 0.6242 - val_loss: 1.0273 - val_accuracy: 0.6188\n",
            "Epoch 22/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.0204 - accuracy: 0.6323 - val_loss: 1.0585 - val_accuracy: 0.6119\n",
            "Epoch 23/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 1.0035 - accuracy: 0.6390 - val_loss: 1.0268 - val_accuracy: 0.6152\n",
            "Epoch 24/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.9719 - accuracy: 0.6521 - val_loss: 1.0343 - val_accuracy: 0.6191\n",
            "Epoch 25/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.9591 - accuracy: 0.6535 - val_loss: 1.0810 - val_accuracy: 0.5971\n",
            "Epoch 26/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.9497 - accuracy: 0.6555 - val_loss: 1.0015 - val_accuracy: 0.6269\n",
            "Epoch 27/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.9308 - accuracy: 0.6636 - val_loss: 1.1470 - val_accuracy: 0.5840\n",
            "Epoch 28/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.9089 - accuracy: 0.6764 - val_loss: 1.0211 - val_accuracy: 0.6213\n",
            "Epoch 29/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.8833 - accuracy: 0.6822 - val_loss: 1.0211 - val_accuracy: 0.6336\n",
            "Epoch 30/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.8669 - accuracy: 0.6873 - val_loss: 1.0353 - val_accuracy: 0.6330\n",
            "Epoch 31/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.8550 - accuracy: 0.6960 - val_loss: 0.9995 - val_accuracy: 0.6422\n",
            "Epoch 32/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.8415 - accuracy: 0.6993 - val_loss: 1.0525 - val_accuracy: 0.6269\n",
            "Epoch 33/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.8221 - accuracy: 0.7079 - val_loss: 0.9759 - val_accuracy: 0.6528\n",
            "Epoch 34/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.7983 - accuracy: 0.7159 - val_loss: 0.9996 - val_accuracy: 0.6517\n",
            "Epoch 35/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.7897 - accuracy: 0.7182 - val_loss: 0.9985 - val_accuracy: 0.6470\n",
            "Epoch 36/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.7802 - accuracy: 0.7220 - val_loss: 0.9953 - val_accuracy: 0.6495\n",
            "Epoch 37/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.7679 - accuracy: 0.7275 - val_loss: 0.9901 - val_accuracy: 0.6509\n",
            "Epoch 38/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.7488 - accuracy: 0.7335 - val_loss: 0.9930 - val_accuracy: 0.6498\n",
            "Epoch 39/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.7344 - accuracy: 0.7404 - val_loss: 1.0111 - val_accuracy: 0.6559\n",
            "Epoch 40/100\n",
            "32298/32298 [==============================] - 78s 2ms/step - loss: 0.7140 - accuracy: 0.7458 - val_loss: 1.0290 - val_accuracy: 0.6556\n",
            "Epoch 41/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.7039 - accuracy: 0.7508 - val_loss: 0.9945 - val_accuracy: 0.6604\n",
            "Epoch 42/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.6995 - accuracy: 0.7561 - val_loss: 1.0030 - val_accuracy: 0.6559\n",
            "Epoch 43/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.6791 - accuracy: 0.7617 - val_loss: 1.0001 - val_accuracy: 0.6587\n",
            "Epoch 44/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.6711 - accuracy: 0.7671 - val_loss: 1.0338 - val_accuracy: 0.6397\n",
            "Epoch 45/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.6646 - accuracy: 0.7710 - val_loss: 1.0206 - val_accuracy: 0.6604\n",
            "Epoch 46/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.6496 - accuracy: 0.7736 - val_loss: 1.0115 - val_accuracy: 0.6590\n",
            "Epoch 47/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.6410 - accuracy: 0.7771 - val_loss: 1.0339 - val_accuracy: 0.6684\n",
            "Epoch 48/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.6189 - accuracy: 0.7839 - val_loss: 1.0549 - val_accuracy: 0.6581\n",
            "Epoch 49/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.6142 - accuracy: 0.7892 - val_loss: 1.1068 - val_accuracy: 0.6461\n",
            "Epoch 50/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5993 - accuracy: 0.7913 - val_loss: 1.0528 - val_accuracy: 0.6659\n",
            "Epoch 51/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5911 - accuracy: 0.7967 - val_loss: 1.1042 - val_accuracy: 0.6612\n",
            "Epoch 52/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5693 - accuracy: 0.8034 - val_loss: 1.0330 - val_accuracy: 0.6673\n",
            "Epoch 53/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5692 - accuracy: 0.8026 - val_loss: 1.1179 - val_accuracy: 0.6606\n",
            "Epoch 54/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5565 - accuracy: 0.8090 - val_loss: 1.0618 - val_accuracy: 0.6609\n",
            "Epoch 55/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5499 - accuracy: 0.8102 - val_loss: 1.0748 - val_accuracy: 0.6695\n",
            "Epoch 56/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5351 - accuracy: 0.8158 - val_loss: 1.1378 - val_accuracy: 0.6556\n",
            "Epoch 57/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5294 - accuracy: 0.8215 - val_loss: 1.1021 - val_accuracy: 0.6581\n",
            "Epoch 58/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5218 - accuracy: 0.8199 - val_loss: 1.1472 - val_accuracy: 0.6581\n",
            "Epoch 59/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5124 - accuracy: 0.8257 - val_loss: 1.1032 - val_accuracy: 0.6648\n",
            "Epoch 60/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5027 - accuracy: 0.8309 - val_loss: 1.0764 - val_accuracy: 0.6570\n",
            "Epoch 61/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5007 - accuracy: 0.8313 - val_loss: 1.1663 - val_accuracy: 0.6693\n",
            "Epoch 62/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.5007 - accuracy: 0.8324 - val_loss: 1.1111 - val_accuracy: 0.6629\n",
            "Epoch 63/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4881 - accuracy: 0.8367 - val_loss: 1.2334 - val_accuracy: 0.6403\n",
            "Epoch 64/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4750 - accuracy: 0.8390 - val_loss: 1.1428 - val_accuracy: 0.6631\n",
            "Epoch 65/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4737 - accuracy: 0.8430 - val_loss: 1.1562 - val_accuracy: 0.6581\n",
            "Epoch 66/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4550 - accuracy: 0.8461 - val_loss: 1.2007 - val_accuracy: 0.6489\n",
            "Epoch 67/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4561 - accuracy: 0.8468 - val_loss: 1.1994 - val_accuracy: 0.6640\n",
            "Epoch 68/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4553 - accuracy: 0.8484 - val_loss: 1.1929 - val_accuracy: 0.6578\n",
            "Epoch 69/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4402 - accuracy: 0.8536 - val_loss: 1.1875 - val_accuracy: 0.6587\n",
            "Epoch 70/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4478 - accuracy: 0.8502 - val_loss: 1.1296 - val_accuracy: 0.6654\n",
            "Epoch 71/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4271 - accuracy: 0.8574 - val_loss: 1.1532 - val_accuracy: 0.6637\n",
            "Epoch 72/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4276 - accuracy: 0.8561 - val_loss: 1.1556 - val_accuracy: 0.6737\n",
            "Epoch 73/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4193 - accuracy: 0.8606 - val_loss: 1.1774 - val_accuracy: 0.6617\n",
            "Epoch 74/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4175 - accuracy: 0.8616 - val_loss: 1.2890 - val_accuracy: 0.6656\n",
            "Epoch 75/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4148 - accuracy: 0.8618 - val_loss: 1.2387 - val_accuracy: 0.6746\n",
            "Epoch 76/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4021 - accuracy: 0.8690 - val_loss: 1.3332 - val_accuracy: 0.6656\n",
            "Epoch 77/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.4027 - accuracy: 0.8669 - val_loss: 1.2514 - val_accuracy: 0.6659\n",
            "Epoch 78/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3922 - accuracy: 0.8717 - val_loss: 1.2257 - val_accuracy: 0.6715\n",
            "Epoch 79/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3868 - accuracy: 0.8704 - val_loss: 1.2304 - val_accuracy: 0.6637\n",
            "Epoch 80/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3791 - accuracy: 0.8761 - val_loss: 1.3309 - val_accuracy: 0.6551\n",
            "Epoch 81/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3746 - accuracy: 0.8762 - val_loss: 1.2583 - val_accuracy: 0.6551\n",
            "Epoch 82/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3677 - accuracy: 0.8795 - val_loss: 1.2769 - val_accuracy: 0.6612\n",
            "Epoch 83/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3653 - accuracy: 0.8823 - val_loss: 1.2615 - val_accuracy: 0.6690\n",
            "Epoch 84/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3576 - accuracy: 0.8829 - val_loss: 1.2923 - val_accuracy: 0.6698\n",
            "Epoch 85/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3609 - accuracy: 0.8832 - val_loss: 1.2727 - val_accuracy: 0.6623\n",
            "Epoch 86/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3439 - accuracy: 0.8899 - val_loss: 1.3082 - val_accuracy: 0.6506\n",
            "Epoch 87/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3554 - accuracy: 0.8854 - val_loss: 1.3323 - val_accuracy: 0.6617\n",
            "Epoch 88/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3446 - accuracy: 0.8881 - val_loss: 1.2704 - val_accuracy: 0.6528\n",
            "Epoch 89/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3449 - accuracy: 0.8899 - val_loss: 1.2630 - val_accuracy: 0.6665\n",
            "Epoch 90/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3380 - accuracy: 0.8878 - val_loss: 1.4344 - val_accuracy: 0.6578\n",
            "Epoch 91/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3340 - accuracy: 0.8936 - val_loss: 1.3466 - val_accuracy: 0.6609\n",
            "Epoch 92/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3240 - accuracy: 0.8950 - val_loss: 1.4038 - val_accuracy: 0.6656\n",
            "Epoch 93/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3219 - accuracy: 0.8978 - val_loss: 1.4231 - val_accuracy: 0.6590\n",
            "Epoch 94/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3238 - accuracy: 0.8970 - val_loss: 1.2860 - val_accuracy: 0.6629\n",
            "Epoch 95/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3180 - accuracy: 0.8997 - val_loss: 1.4087 - val_accuracy: 0.6637\n",
            "Epoch 96/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3205 - accuracy: 0.8975 - val_loss: 1.3034 - val_accuracy: 0.6648\n",
            "Epoch 97/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3227 - accuracy: 0.8962 - val_loss: 1.3458 - val_accuracy: 0.6578\n",
            "Epoch 98/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3014 - accuracy: 0.9041 - val_loss: 1.4448 - val_accuracy: 0.6662\n",
            "Epoch 99/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3081 - accuracy: 0.9024 - val_loss: 1.2942 - val_accuracy: 0.6687\n",
            "Epoch 100/100\n",
            "32298/32298 [==============================] - 77s 2ms/step - loss: 0.3004 - accuracy: 0.9054 - val_loss: 1.4006 - val_accuracy: 0.6651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f707ee6d550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8-f2m25AxJC",
        "colab_type": "text"
      },
      "source": [
        "Проверяем работоспособность модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dwNbJw-BCOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_index_of_max(arr):\n",
        "  return np.where(arr == max(arr))[0][0]\n",
        "\n",
        "def get_emotion(index):\n",
        "  for id, emotion in emotions:\n",
        "    if (id == index):\n",
        "      return emotion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEfg-j0BAwjJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7752a6c0-d823-4f0e-ace0-faf37e2fd9b0"
      },
      "source": [
        "image = np.array([test_images[2]])\n",
        "predicted = model.predict(image)[0]\n",
        "get_emotion(get_index_of_max(predicted))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FEAR'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz6zvlnKA0dx",
        "colab_type": "text"
      },
      "source": [
        "Сохраняем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWxSKK1PTgW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"fer_model.tf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H1yDc9QA2xE",
        "colab_type": "text"
      },
      "source": [
        "Загружаем модель с диска (если она у вас не загружена)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxird3p2vguF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Укажите свой путь до обученной модели\n",
        "model_path = \"/content/drive/My Drive/university/reko/fer_model.tf\"\n",
        "model = load_model(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XpBF7MKS8fe",
        "colab_type": "text"
      },
      "source": [
        "Так же можно конвертировать Keras модель в tflite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcF4EUTeS2JK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "47d74ddb-8f85-4b51-a6b3-c421875cc521"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tfmodel = converter.convert()\n",
        "open(\"model.tflite\", \"wb\").write(tfmodel)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-d1e571ee8584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtfmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.tflite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mfrom_keras_model\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m    430\u001b[0m       \u001b[0;31m# signature including the batch dimension specified by the user.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m       input_signature = _saving_utils.model_input_signature(\n\u001b[0;32m--> 432\u001b[0;31m           model, keep_original_batch_size=True)\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_saving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_model_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_input_signature\u001b[0;34m(model, keep_original_batch_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mTensorSpecs\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m   \u001b[0minput_specs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mkeep_original_batch_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_specs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_get_save_spec'"
          ]
        }
      ]
    }
  ]
}